# Story 1.2: Database Schema Creation

**Status:** review
**Epic:** 1 - Foundation & Authentication
**Story ID:** 1.2
**Created:** 2026-01-08

---

## Story

As a **system**,
I want **PostgreSQL database tables created with proper indexes and relationships**,
So that **trend data, collections, users, and quota tracking can be persisted**.

---

## Acceptance Criteria

**Given** Railway PostgreSQL database is provisioned
**When** Alembic migrations are executed
**Then** `trends` table is created with columns: id (UUID PRIMARY KEY), title (VARCHAR 500), collection_id (UUID FK), created_at (TIMESTAMP), reddit_score (INTEGER), reddit_comments (INTEGER), reddit_upvote_ratio (FLOAT), reddit_subreddit (VARCHAR 100), youtube_views (INTEGER), youtube_likes (INTEGER), youtube_channel (VARCHAR 200), google_trends_interest (INTEGER), google_trends_related_queries (JSONB), similarweb_traffic (INTEGER), similarweb_sources (JSONB), reddit_velocity_score (FLOAT), youtube_traction_score (FLOAT), google_trends_spike_score (FLOAT), similarweb_bonus_applied (BOOLEAN), momentum_score (FLOAT), confidence_level (VARCHAR 10), ai_brief (TEXT), ai_brief_generated_at (TIMESTAMP)
**And** `data_collections` table is created with: id (UUID PRIMARY KEY), started_at (TIMESTAMP), completed_at (TIMESTAMP), status (VARCHAR 20), errors (JSONB), reddit_api_calls (INTEGER), youtube_api_quota_used (INTEGER), google_trends_api_calls (INTEGER)
**And** `users` table is created with: id (UUID PRIMARY KEY), username (VARCHAR 100 UNIQUE), password_hash (VARCHAR 255), created_at (TIMESTAMP), last_login (TIMESTAMP)
**And** `api_quota_usage` table is created with: id (SERIAL PRIMARY KEY), api_name (VARCHAR 50), date (DATE), units_used (INTEGER), UNIQUE(api_name, date)
**And** Indexes are created on: trends.momentum_score DESC, trends.confidence_level, trends.created_at DESC, data_collections.started_at DESC
**And** Foreign key relationship created: trends.collection_id â†’ data_collections.id
**And** Check constraint on trends.confidence_level IN ('high', 'medium', 'low')
**And** Check constraint on data_collections.status IN ('in_progress', 'completed', 'failed')

---

## Developer Context & Implementation Guide

### ðŸŽ¯ Epic Context

This story is the **second story** in Epic 1: Foundation & Authentication. It creates the database schema that all subsequent stories will use for data persistence.

**Epic Goal:** Establish secure, deployed infrastructure with authentication, database, and foundational backend/frontend architecture.

**Dependencies:**
- âœ… **Story 1.1 (Project Setup & Railway Deployment)** - COMPLETE
  - Railway PostgreSQL already provisioned
  - DATABASE_URL environment variable configured
  - SQLAlchemy 2.0.23 and psycopg2-binary installed
  - Backend application structure exists

**Dependent Stories (blocked by this story):**
- **Story 1.3 (Backend Authentication)** - Needs `users` table
- **Story 2.1 (API Collector Infrastructure)** - Needs `data_collections` and `api_quota_usage` tables
- **Story 2.2-2.5 (Data Collection)** - Needs `trends` table

---

## Technical Requirements

### Architecture Decision References

This story implements **AD-3: PostgreSQL for Data Persistence**:

#### Database Technology Stack
- **Database:** PostgreSQL 14+ (managed by Railway)
- **ORM:** SQLAlchemy 2.0.23 with async support
- **Migrations:** Alembic (latest stable)
- **Connection:** Via Railway-provided `DATABASE_URL` environment variable
- **Async Driver:** asyncpg (psycopg2-binary already installed for sync fallback)

#### Schema Design Principles (from Architecture Document)
1. **UUID Primary Keys** - For trends, data_collections, users tables (better for distributed systems)
2. **JSONB Columns** - For flexible API response metadata (Google Trends related queries, SimilarWeb sources, etc.)
3. **Strategic Indexes** - On frequently queried columns (momentum_score DESC, created_at DESC, confidence_level)
4. **Referential Integrity** - Foreign key constraints (trends.collection_id â†’ data_collections.id)
5. **Data Integrity** - Check constraints on enum-like columns (confidence_level, status)
6. **Future-Proof** - Schema supports Phase 2 multi-user expansion without migration

### Key Architectural Requirements

**From AD-3 (PostgreSQL for Data Persistence):**
- Use SQLAlchemy ORM to prevent SQL injection via parameterized queries
- JSONB column type for flexible metadata storage
- Support future multi-user expansion through schema design
- Database migrations managed with Alembic
- ACID compliance for data integrity

**From AD-9 (Error Handling):**
- Schema must support tracking API failures (errors JSONB column in data_collections)
- Schema must support partial data collection (nullable columns for API metrics)

**From AD-10 (Observability):**
- Schema must track API quota usage (api_quota_usage table)
- Schema must track collection metrics (api_calls columns in data_collections)

---

## Implementation Tasks

### Task 1: Install and Initialize Alembic

**Acceptance Criteria:** AC #2 (Alembic migrations executed)

**Subtasks:**
- [x] Add Alembic to requirements.txt
- [x] Add asyncpg for async PostgreSQL driver
- [x] Initialize Alembic in backend directory
- [x] Configure Alembic to use async SQLAlchemy
- [x] Configure Alembic env.py to load DATABASE_URL from environment

**Implementation Steps:**

1. **Update requirements.txt** (backend/requirements.txt):
```python
# Add after existing dependencies
alembic==1.13.1
asyncpg==0.29.0
```

2. **Initialize Alembic**:
```bash
cd backend
alembic init alembic
```

This creates:
```
backend/
â”œâ”€â”€ alembic/
â”‚   â”œâ”€â”€ versions/         # Migration files
â”‚   â”œâ”€â”€ env.py            # Alembic configuration
â”‚   â”œâ”€â”€ script.py.mako    # Migration template
â”‚   â””â”€â”€ README
â””â”€â”€ alembic.ini           # Alembic settings
```

3. **Configure alembic.ini** (backend/alembic.ini):
```ini
[alembic]
# Set relative path to migration scripts
script_location = alembic

# Use async driver for PostgreSQL
# Note: sqlalchemy.url will be set programmatically from env variable
# sqlalchemy.url = driver://user:pass@localhost/dbname

# Prepend sys.path for model imports
prepend_sys_path = .

# Timezone for migration timestamps
timezone = UTC

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
```

4. **Configure env.py for async and environment variables** (backend/alembic/env.py):
```python
from logging.config import fileConfig
from sqlalchemy import pool
from sqlalchemy.engine import Connection
from sqlalchemy.ext.asyncio import async_engine_from_config
from alembic import context
import asyncio
import os

# Import your models' Base for autogenerate support
from app.models.base import Base
from app.config import settings

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Set sqlalchemy.url from environment variable
config.set_main_option("sqlalchemy.url", settings.database_url)

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here for 'autogenerate' support
target_metadata = Base.metadata

def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.
    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def do_run_migrations(connection: Connection) -> None:
    context.configure(connection=connection, target_metadata=target_metadata)

    with context.begin_transaction():
        context.run_migrations()


async def run_async_migrations() -> None:
    """Run migrations in 'online' mode with async engine."""
    connectable = async_engine_from_config(
        config.get_section(config.config_ini_section, {}),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    async with connectable.connect() as connection:
        await connection.run_sync(do_run_migrations)

    await connectable.dispose()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode with async support."""
    asyncio.run(run_async_migrations())


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

---

### Task 2: Create SQLAlchemy Models with Async Support

**Acceptance Criteria:** AC #1, #3, #4, #5 (all tables created)

**Subtasks:**
- [x] Create Base model class with async support
- [x] Create Trend model with all platform metrics
- [x] Create DataCollection model for collection tracking
- [x] Create User model for authentication
- [x] Create ApiQuotaUsage model for quota tracking
- [x] Add naming conventions for constraints

**Implementation Steps:**

1. **Create base model** (backend/app/models/base.py):
```python
"""Base model with async support and common utilities."""
from datetime import datetime
from typing import Any
from uuid import UUID, uuid4

from sqlalchemy import MetaData
from sqlalchemy.ext.asyncio import AsyncAttrs
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column
from sqlalchemy.sql import func

# Define naming convention for constraints (Alembic best practice)
convention = {
    "ix": "ix_%(column_0_label)s",
    "uq": "uq_%(table_name)s_%(column_0_name)s",
    "ck": "ck_%(table_name)s_%(constraint_name)s",
    "fk": "fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s",
    "pk": "pk_%(table_name)s"
}

metadata = MetaData(naming_convention=convention)


class Base(AsyncAttrs, DeclarativeBase):
    """Base class for all database models with async support."""
    metadata = metadata

    # Common columns for models with UUID primary keys
    __abstract__ = True


class UUIDMixin:
    """Mixin for models with UUID primary key."""
    id: Mapped[UUID] = mapped_column(primary_key=True, default=uuid4)


class TimestampMixin:
    """Mixin for models with created_at timestamp."""
    created_at: Mapped[datetime] = mapped_column(
        server_default=func.now(),
        nullable=False
    )
```

2. **Create Trend model** (backend/app/models/trend.py):
```python
"""Trend model - stores collected trend data and calculated scores."""
from datetime import datetime
from typing import Optional
from uuid import UUID

from sqlalchemy import (
    Boolean, CheckConstraint, Float, ForeignKey, Index, Integer,
    String, Text, TIMESTAMP
)
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import Mapped, mapped_column, relationship

from .base import Base, UUIDMixin, TimestampMixin


class Trend(Base, UUIDMixin, TimestampMixin):
    """Trend data from cross-platform collection."""

    __tablename__ = "trends"

    # Basic Info
    title: Mapped[str] = mapped_column(String(500), nullable=False)
    collection_id: Mapped[UUID] = mapped_column(
        ForeignKey("data_collections.id", ondelete="CASCADE"),
        nullable=False,
        index=True
    )

    # Reddit Metrics (raw)
    reddit_score: Mapped[Optional[int]] = mapped_column(Integer, nullable=True)
    reddit_comments: Mapped[Optional[int]] = mapped_column(Integer, nullable=True)
    reddit_upvote_ratio: Mapped[Optional[float]] = mapped_column(Float, nullable=True)
    reddit_subreddit: Mapped[Optional[str]] = mapped_column(String(100), nullable=True)

    # YouTube Metrics (raw)
    youtube_views: Mapped[Optional[int]] = mapped_column(Integer, nullable=True)
    youtube_likes: Mapped[Optional[int]] = mapped_column(Integer, nullable=True)
    youtube_channel: Mapped[Optional[str]] = mapped_column(String(200), nullable=True)

    # Google Trends Metrics (raw)
    google_trends_interest: Mapped[Optional[int]] = mapped_column(Integer, nullable=True)
    google_trends_related_queries: Mapped[Optional[dict]] = mapped_column(JSONB, nullable=True)

    # SimilarWeb Metrics (raw)
    similarweb_traffic: Mapped[Optional[int]] = mapped_column(Integer, nullable=True)
    similarweb_sources: Mapped[Optional[dict]] = mapped_column(JSONB, nullable=True)

    # Normalized Scores (0-100 scale)
    reddit_velocity_score: Mapped[Optional[float]] = mapped_column(Float, nullable=True)
    youtube_traction_score: Mapped[Optional[float]] = mapped_column(Float, nullable=True)
    google_trends_spike_score: Mapped[Optional[float]] = mapped_column(Float, nullable=True)
    similarweb_bonus_applied: Mapped[Optional[bool]] = mapped_column(Boolean, default=False)

    # Composite Momentum Score
    momentum_score: Mapped[Optional[float]] = mapped_column(Float, nullable=True, index=True)
    confidence_level: Mapped[Optional[str]] = mapped_column(
        String(10),
        CheckConstraint("confidence_level IN ('high', 'medium', 'low')", name="valid_confidence"),
        nullable=True,
        index=True
    )

    # AI Generated Brief
    ai_brief: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
    ai_brief_generated_at: Mapped[Optional[datetime]] = mapped_column(TIMESTAMP, nullable=True)

    # Relationships
    collection: Mapped["DataCollection"] = relationship("DataCollection", back_populates="trends")

    # Indexes for query performance
    __table_args__ = (
        Index("idx_momentum_score_desc", momentum_score.desc()),
        Index("idx_created_at_desc", "created_at", postgresql_ops={"created_at": "DESC"}),
        Index("idx_confidence_level", confidence_level),
    )

    def __repr__(self) -> str:
        return f"<Trend(id={self.id}, title='{self.title[:50]}...', momentum_score={self.momentum_score})>"
```

3. **Create DataCollection model** (backend/app/models/data_collection.py):
```python
"""DataCollection model - tracks collection runs and API usage."""
from datetime import datetime
from typing import List, Optional
from uuid import UUID

from sqlalchemy import CheckConstraint, Index, Integer, TIMESTAMP
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import Mapped, mapped_column, relationship

from .base import Base, UUIDMixin


class DataCollection(Base, UUIDMixin):
    """Data collection run metadata and API usage tracking."""

    __tablename__ = "data_collections"

    # Collection timestamps
    started_at: Mapped[datetime] = mapped_column(
        TIMESTAMP,
        server_default=func.now(),
        nullable=False,
        index=True
    )
    completed_at: Mapped[Optional[datetime]] = mapped_column(TIMESTAMP, nullable=True)

    # Collection status
    status: Mapped[str] = mapped_column(
        String(20),
        CheckConstraint("status IN ('in_progress', 'completed', 'failed')", name="valid_status"),
        nullable=False,
        default="in_progress"
    )

    # Error tracking (JSONB for flexibility)
    errors: Mapped[Optional[dict]] = mapped_column(JSONB, nullable=True)

    # API usage metrics
    reddit_api_calls: Mapped[Optional[int]] = mapped_column(Integer, nullable=True, default=0)
    youtube_api_quota_used: Mapped[Optional[int]] = mapped_column(Integer, nullable=True, default=0)
    google_trends_api_calls: Mapped[Optional[int]] = mapped_column(Integer, nullable=True, default=0)

    # Relationships
    trends: Mapped[List["Trend"]] = relationship(
        "Trend",
        back_populates="collection",
        cascade="all, delete-orphan"
    )

    # Indexes
    __table_args__ = (
        Index("idx_started_at_desc", "started_at", postgresql_ops={"started_at": "DESC"}),
    )

    def __repr__(self) -> str:
        return f"<DataCollection(id={self.id}, status='{self.status}', started_at={self.started_at})>"
```

4. **Create User model** (backend/app/models/user.py):
```python
"""User model - authentication and user management."""
from datetime import datetime
from typing import Optional

from sqlalchemy import String, TIMESTAMP
from sqlalchemy.orm import Mapped, mapped_column

from .base import Base, UUIDMixin, TimestampMixin


class User(Base, UUIDMixin, TimestampMixin):
    """User account for authentication."""

    __tablename__ = "users"

    # Authentication
    username: Mapped[str] = mapped_column(String(100), unique=True, nullable=False, index=True)
    password_hash: Mapped[str] = mapped_column(String(255), nullable=False)

    # Activity tracking
    last_login: Mapped[Optional[datetime]] = mapped_column(TIMESTAMP, nullable=True)

    def __repr__(self) -> str:
        return f"<User(id={self.id}, username='{self.username}')>"
```

5. **Create ApiQuotaUsage model** (backend/app/models/api_quota_usage.py):
```python
"""ApiQuotaUsage model - tracks daily API quota consumption."""
from datetime import date

from sqlalchemy import Date, Integer, String, UniqueConstraint
from sqlalchemy.orm import Mapped, mapped_column

from .base import Base


class ApiQuotaUsage(Base):
    """Daily API quota usage tracking."""

    __tablename__ = "api_quota_usage"

    # Use SERIAL for auto-incrementing integer PK (not UUID)
    id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True)

    # API identification
    api_name: Mapped[str] = mapped_column(String(50), nullable=False)

    # Date tracking
    date: Mapped[date] = mapped_column(Date, nullable=False)

    # Quota usage
    units_used: Mapped[int] = mapped_column(Integer, nullable=False, default=0)

    # Ensure one record per API per day
    __table_args__ = (
        UniqueConstraint("api_name", "date", name="uq_api_quota_usage_api_name_date"),
    )

    def __repr__(self) -> str:
        return f"<ApiQuotaUsage(api='{self.api_name}', date={self.date}, units={self.units_used})>"
```

6. **Create models/__init__.py** (backend/app/models/__init__.py):
```python
"""SQLAlchemy models for trend-monitor database."""
from .base import Base
from .trend import Trend
from .data_collection import DataCollection
from .user import User
from .api_quota_usage import ApiQuotaUsage

__all__ = [
    "Base",
    "Trend",
    "DataCollection",
    "User",
    "ApiQuotaUsage",
]
```

---

### Task 3: Create Initial Migration

**Acceptance Criteria:** AC #2 (Alembic migrations executed)

**Subtasks:**
- [x] Generate initial migration with autogenerate (manually created due to db connection issues)
- [x] Review and verify migration script
- [x] Add missing imports if needed
- [x] Test migration locally (skipped - tested on Railway directly)

**Implementation Steps:**

1. **Generate initial migration**:
```bash
cd backend
alembic revision --autogenerate -m "Initial schema: trends, data_collections, users, api_quota_usage"
```

This creates: `backend/alembic/versions/XXXX_initial_schema.py`

2. **Review generated migration** (backend/alembic/versions/XXXX_initial_schema.py):

Alembic autogenerate should create something like:
```python
"""Initial schema: trends, data_collections, users, api_quota_usage

Revision ID: 001_abcdef123456
Revises:
Create Date: 2026-01-08 10:00:00.000000

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = '001_abcdef123456'
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###

    # Create data_collections table first (referenced by trends FK)
    op.create_table('data_collections',
        sa.Column('id', sa.UUID(), nullable=False),
        sa.Column('started_at', sa.TIMESTAMP(), server_default=sa.text('now()'), nullable=False),
        sa.Column('completed_at', sa.TIMESTAMP(), nullable=True),
        sa.Column('status', sa.String(length=20), nullable=False),
        sa.Column('errors', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column('reddit_api_calls', sa.Integer(), nullable=True),
        sa.Column('youtube_api_quota_used', sa.Integer(), nullable=True),
        sa.Column('google_trends_api_calls', sa.Integer(), nullable=True),
        sa.CheckConstraint("status IN ('in_progress', 'completed', 'failed')", name='ck_data_collections_valid_status'),
        sa.PrimaryKeyConstraint('id', name=op.f('pk_data_collections'))
    )
    op.create_index('idx_started_at_desc', 'data_collections', ['started_at'], unique=False, postgresql_ops={'started_at': 'DESC'})

    # Create trends table
    op.create_table('trends',
        sa.Column('id', sa.UUID(), nullable=False),
        sa.Column('title', sa.String(length=500), nullable=False),
        sa.Column('collection_id', sa.UUID(), nullable=False),
        sa.Column('created_at', sa.TIMESTAMP(), server_default=sa.text('now()'), nullable=False),
        sa.Column('reddit_score', sa.Integer(), nullable=True),
        sa.Column('reddit_comments', sa.Integer(), nullable=True),
        sa.Column('reddit_upvote_ratio', sa.Float(), nullable=True),
        sa.Column('reddit_subreddit', sa.String(length=100), nullable=True),
        sa.Column('youtube_views', sa.Integer(), nullable=True),
        sa.Column('youtube_likes', sa.Integer(), nullable=True),
        sa.Column('youtube_channel', sa.String(length=200), nullable=True),
        sa.Column('google_trends_interest', sa.Integer(), nullable=True),
        sa.Column('google_trends_related_queries', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column('similarweb_traffic', sa.Integer(), nullable=True),
        sa.Column('similarweb_sources', postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column('reddit_velocity_score', sa.Float(), nullable=True),
        sa.Column('youtube_traction_score', sa.Float(), nullable=True),
        sa.Column('google_trends_spike_score', sa.Float(), nullable=True),
        sa.Column('similarweb_bonus_applied', sa.Boolean(), nullable=True),
        sa.Column('momentum_score', sa.Float(), nullable=True),
        sa.Column('confidence_level', sa.String(length=10), nullable=True),
        sa.Column('ai_brief', sa.Text(), nullable=True),
        sa.Column('ai_brief_generated_at', sa.TIMESTAMP(), nullable=True),
        sa.CheckConstraint("confidence_level IN ('high', 'medium', 'low')", name='ck_trends_valid_confidence'),
        sa.ForeignKeyConstraint(['collection_id'], ['data_collections.id'], name=op.f('fk_trends_collection_id_data_collections'), ondelete='CASCADE'),
        sa.PrimaryKeyConstraint('id', name=op.f('pk_trends'))
    )
    op.create_index('idx_confidence_level', 'trends', ['confidence_level'], unique=False)
    op.create_index('idx_created_at_desc', 'trends', ['created_at'], unique=False, postgresql_ops={'created_at': 'DESC'})
    op.create_index('idx_momentum_score_desc', 'trends', [sa.text('momentum_score DESC')], unique=False)
    op.create_index(op.f('ix_collection_id'), 'trends', ['collection_id'], unique=False)
    op.create_index(op.f('ix_momentum_score'), 'trends', ['momentum_score'], unique=False)

    # Create users table
    op.create_table('users',
        sa.Column('id', sa.UUID(), nullable=False),
        sa.Column('username', sa.String(length=100), nullable=False),
        sa.Column('password_hash', sa.String(length=255), nullable=False),
        sa.Column('created_at', sa.TIMESTAMP(), server_default=sa.text('now()'), nullable=False),
        sa.Column('last_login', sa.TIMESTAMP(), nullable=True),
        sa.PrimaryKeyConstraint('id', name=op.f('pk_users')),
        sa.UniqueConstraint('username', name=op.f('uq_users_username'))
    )
    op.create_index(op.f('ix_username'), 'users', ['username'], unique=False)

    # Create api_quota_usage table
    op.create_table('api_quota_usage',
        sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
        sa.Column('api_name', sa.String(length=50), nullable=False),
        sa.Column('date', sa.Date(), nullable=False),
        sa.Column('units_used', sa.Integer(), nullable=False),
        sa.PrimaryKeyConstraint('id', name=op.f('pk_api_quota_usage')),
        sa.UniqueConstraint('api_name', 'date', name='uq_api_quota_usage_api_name_date')
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('api_quota_usage')
    op.drop_table('users')
    op.drop_index(op.f('ix_momentum_score'), table_name='trends')
    op.drop_index(op.f('ix_collection_id'), table_name='trends')
    op.drop_index('idx_momentum_score_desc', table_name='trends')
    op.drop_index('idx_created_at_desc', table_name='trends')
    op.drop_index('idx_confidence_level', table_name='trends')
    op.drop_table('trends')
    op.drop_index('idx_started_at_desc', table_name='data_collections')
    op.drop_table('data_collections')
    # ### end Alembic commands ###
```

**IMPORTANT:** Manually review the generated migration and verify:
- All tables created in correct order (data_collections before trends due to FK)
- All indexes created
- All check constraints created
- All foreign keys created with CASCADE on delete
- Downgrade reverses upgrade correctly

---

### Task 4: Run Migration on Railway PostgreSQL

**Acceptance Criteria:** AC #1-8 (all tables, indexes, constraints created)

**Subtasks:**
- [x] Test migration locally (optional - skipped)
- [x] Run migration on Railway database (configured via Dockerfile CMD)
- [x] Verify schema in Railway database (verified via /debug/schema endpoint)
- [x] Create bootstrap user for MVP (completed - user 'dave' exists)

**Implementation Steps:**

1. **Test migration locally (optional)**:
```bash
# Create local PostgreSQL database for testing
createdb trend_monitor_dev

# Export local DATABASE_URL
export DATABASE_URL="postgresql://localhost/trend_monitor_dev"

# Run migration
cd backend
alembic upgrade head

# Verify tables created
psql trend_monitor_dev -c "\dt"
```

2. **Run migration on Railway**:

Option A: Via Railway CLI
```bash
# Ensure Railway CLI authenticated (from Story 1.1)
railway whoami

# Run migration on Railway database
cd backend
railway run alembic upgrade head
```

Option B: Via deployment (automatic on push)
- Add migration command to Railway startup
- Update Dockerfile or railway.json to run migrations before starting app

**Recommended: Update Dockerfile** (Dockerfile):
```dockerfile
FROM python:3.10-slim

WORKDIR /app

# Copy and install dependencies
COPY backend/requirements.txt /app/backend/requirements.txt
RUN pip install --no-cache-dir -r backend/requirements.txt

# Copy application code
COPY backend /app/backend

# Expose port
EXPOSE 8000

# Set working directory to backend
WORKDIR /app/backend

# Run migrations then start app
CMD alembic upgrade head && uvicorn app.main:app --host 0.0.0.0 --port ${PORT:-8000}
```

3. **Verify schema created**:
```bash
# Connect to Railway PostgreSQL
railway run psql $DATABASE_URL

# List all tables
\dt

# Expected output:
# trends
# data_collections
# users
# api_quota_usage

# Verify trends table structure
\d trends

# Verify indexes
\di

# Verify foreign keys
SELECT
    tc.table_name,
    kcu.column_name,
    ccu.table_name AS foreign_table_name,
    ccu.column_name AS foreign_column_name
FROM information_schema.table_constraints AS tc
JOIN information_schema.key_column_usage AS kcu
  ON tc.constraint_name = kcu.constraint_name
JOIN information_schema.constraint_column_usage AS ccu
  ON ccu.constraint_name = tc.constraint_name
WHERE tc.constraint_type = 'FOREIGN KEY';

# Verify check constraints
SELECT constraint_name, check_clause
FROM information_schema.check_constraints
WHERE constraint_schema = 'public';
```

4. **Create bootstrap user for MVP**:
```bash
# Create initial user (dave) for MVP
railway run python -c "
from app.models import User
from app.config import settings
from passlib.context import CryptContext
from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker
import asyncio

pwd_context = CryptContext(schemes=['bcrypt'], deprecated='auto')

async def create_user():
    engine = create_async_engine(settings.database_url, echo=True)
    async_session = async_sessionmaker(engine, expire_on_commit=False)

    async with async_session() as session:
        user = User(
            username='dave',
            password_hash=pwd_context.hash('CHANGE_ME_INITIAL_PASSWORD')
        )
        session.add(user)
        await session.commit()
        print(f'User created: {user.username}')

    await engine.dispose()

asyncio.run(create_user())
"
```

---

### Task 5: Create Database Helper Module

**Acceptance Criteria:** All ACs (enables future database operations)

**Subtasks:**
- [x] Create database session manager
- [x] Create async database dependency for FastAPI
- [x] Add database connection test endpoint

**Implementation Steps:**

1. **Create database module** (backend/app/database.py):
```python
"""Database session management and utilities."""
from typing import AsyncGenerator

from sqlalchemy.ext.asyncio import (
    AsyncSession,
    async_sessionmaker,
    create_async_engine,
)
from sqlalchemy.pool import NullPool

from app.config import settings

# Create async engine
# Use NullPool for serverless/short-lived connections (Railway)
engine = create_async_engine(
    settings.database_url,
    echo=settings.debug,  # Log SQL in debug mode
    pool_pre_ping=True,   # Verify connections before using
    poolclass=NullPool,   # No connection pooling for Railway
)

# Create async session factory
AsyncSessionLocal = async_sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False,
    autocommit=False,
    autoflush=False,
)


async def get_db() -> AsyncGenerator[AsyncSession, None]:
    """Dependency for FastAPI routes to get database session.

    Usage:
        @app.get("/trends")
        async def get_trends(db: AsyncSession = Depends(get_db)):
            result = await db.execute(select(Trend))
            trends = result.scalars().all()
            return trends
    """
    async with AsyncSessionLocal() as session:
        try:
            yield session
            await session.commit()
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()


async def init_db() -> None:
    """Initialize database (create all tables).

    NOTE: In production, use Alembic migrations instead.
    This is only for development/testing.
    """
    from app.models.base import Base

    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)


async def close_db() -> None:
    """Close database connections."""
    await engine.dispose()
```

2. **Update config.py to ensure database_url is set** (backend/app/config.py):
```python
from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import Optional


class Settings(BaseSettings):
    # Database
    database_url: Optional[str] = None  # Railway provides this

    # ... rest of settings from Story 1.1 ...

    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False
    )


settings = Settings()
```

3. **Add database test endpoint** (backend/app/main.py):
```python
# Add to existing main.py from Story 1.1

from fastapi import Depends
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

from app.database import get_db

# ... existing middleware and routes ...

@app.get("/health/db")
async def health_check_database(db: AsyncSession = Depends(get_db)):
    """Health check endpoint that verifies database connectivity."""
    try:
        # Execute simple query
        result = await db.execute(text("SELECT 1"))
        result.scalar()

        # Check if tables exist
        tables_result = await db.execute(text("""
            SELECT table_name
            FROM information_schema.tables
            WHERE table_schema = 'public'
        """))
        tables = [row[0] for row in tables_result.fetchall()]

        return {
            "status": "healthy",
            "database": "connected",
            "tables": tables,
            "expected_tables": ["trends", "data_collections", "users", "api_quota_usage"],
            "tables_exist": all(t in tables for t in ["trends", "data_collections", "users", "api_quota_usage"])
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "database": "connection_failed",
            "error": str(e)
        }
```

4. **Add lifecycle events for database** (backend/app/main.py):
```python
from contextlib import asynccontextmanager

from app.database import close_db

@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI lifespan event handler for startup/shutdown tasks."""
    # Startup
    print("Starting up - database engine created")
    yield
    # Shutdown
    await close_db()
    print("Shutting down - database connections closed")


# Update FastAPI app initialization
app = FastAPI(
    title="trend-monitor API",
    description="Quantified trend monitoring system API",
    version=settings.app_version,
    lifespan=lifespan  # Add lifespan handler
)
```

---

## Architecture Compliance

### Database Design Compliance

âœ… **AD-3: PostgreSQL for Data Persistence**
- PostgreSQL 14+ with managed Railway hosting
- SQLAlchemy 2.0 ORM with async support
- JSONB columns for flexible metadata (trends.google_trends_related_queries, trends.similarweb_sources, data_collections.errors)
- UUID primary keys for distributed-ready design
- Indexes on frequently queried columns
- Foreign key constraints with CASCADE delete

### Data Integrity

âœ… **Check Constraints:**
- `trends.confidence_level` must be 'high', 'medium', or 'low'
- `data_collections.status` must be 'in_progress', 'completed', or 'failed'

âœ… **Foreign Key Constraints:**
- `trends.collection_id` references `data_collections.id` with CASCADE delete
- Ensures referential integrity between trends and their collection runs

âœ… **Unique Constraints:**
- `users.username` must be unique
- `api_quota_usage` (api_name, date) must be unique per day

### Performance Optimization

âœ… **Strategic Indexes:**
- `idx_momentum_score_desc` - For Top 10 trends query (ORDER BY momentum_score DESC)
- `idx_created_at_desc` - For recent trends query
- `idx_confidence_level` - For filtering by confidence
- `idx_started_at_desc` - For recent collections query
- Foreign key index on `trends.collection_id` for join performance

### Security

âœ… **SQL Injection Prevention:**
- SQLAlchemy ORM uses parameterized queries automatically
- No raw SQL with string concatenation

âœ… **Password Security:**
- `password_hash` column stores bcrypt hash (never plaintext)
- Password hashing implemented in Story 1.3

---

## Library & Framework Requirements

### Required Packages

| Library | Version | Purpose |
|---------|---------|---------|
| sqlalchemy | 2.0.23 | ORM and query builder |
| psycopg2-binary | 2.9.9 | PostgreSQL driver (sync) |
| asyncpg | 0.29.0 | PostgreSQL driver (async) |
| alembic | 1.13.1 | Database migrations |
| pydantic | 2.5.0 | Data validation |

### Why These Versions?

- **SQLAlchemy 2.0.23**: Modern async support with `AsyncAttrs` and `DeclarativeBase`
- **asyncpg 0.29.0**: Fast async PostgreSQL driver, required for SQLAlchemy async operations
- **Alembic 1.13.1**: Latest stable with SQLAlchemy 2.0 support
- **psycopg2-binary 2.9.9**: Sync fallback driver, already installed from Story 1.1

### Latest Best Practices (2026)

**From Web Research:**

1. **Use AsyncAttrs for Async Models** ([Litestar SQLAlchemy Docs](https://docs.litestar.dev/2/usage/databases/sqlalchemy/models_and_repository.html))
   - Inherit from `AsyncAttrs` to enable async attribute loading
   - Use `mapped_column` with type annotations

2. **UUID Generation** ([SQLAlchemy GitHub Discussion](https://github.com/sqlalchemy/sqlalchemy/discussions/10698))
   - Use `default=uuid4` for application-side generation
   - Or use `server_default=text("uuid_generate_v4()")` for database-side generation
   - Application-side is simpler and doesn't require PostgreSQL uuid-ossp extension

3. **Alembic Autogenerate** ([DEV Community Best Practices](https://dev.to/welel/best-practices-for-alembic-and-sqlalchemy-3b34))
   - Use `--autogenerate` for migration generation
   - ALWAYS manually review generated migrations
   - Test migrations with rollback before production

4. **Naming Conventions** ([Alembic Documentation](https://alembic.sqlalchemy.org/en/latest/autogenerate.html))
   - Define constraint naming conventions in MetaData
   - Enables Alembic to properly detect and drop constraints

---

## File Structure Requirements

### Backend Directory Structure (After This Story)
```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                      # FastAPI app (from Story 1.1, updated)
â”‚   â”œâ”€â”€ config.py                    # Settings (from Story 1.1, updated)
â”‚   â”œâ”€â”€ database.py                  # NEW: Database session management
â”‚   â”œâ”€â”€ models/                      # NEW: SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py                  # Base model with async support
â”‚   â”‚   â”œâ”€â”€ trend.py                 # Trend model
â”‚   â”‚   â”œâ”€â”€ data_collection.py       # DataCollection model
â”‚   â”‚   â”œâ”€â”€ user.py                  # User model
â”‚   â”‚   â””â”€â”€ api_quota_usage.py       # ApiQuotaUsage model
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ health.py                # Health endpoints (optional)
â”‚   â””â”€â”€ schemas/                     # Pydantic schemas (Story 1.3+)
â”œâ”€â”€ alembic/                         # NEW: Alembic migrations
â”‚   â”œâ”€â”€ versions/
â”‚   â”‚   â””â”€â”€ 001_initial_schema.py    # Initial migration
â”‚   â”œâ”€â”€ env.py                       # Alembic config (async support)
â”‚   â”œâ”€â”€ script.py.mako
â”‚   â””â”€â”€ README
â”œâ”€â”€ alembic.ini                      # NEW: Alembic settings
â”œâ”€â”€ requirements.txt                 # Updated with alembic, asyncpg
â”œâ”€â”€ pytest.ini                       # From Story 1.1
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test_health.py               # From Story 1.1
```

---

## Testing Requirements

### Manual Testing Checklist
- [ ] Alembic migration runs without errors
- [ ] All 4 tables created (trends, data_collections, users, api_quota_usage)
- [ ] All indexes created (verify with `\di`)
- [ ] Foreign key constraint created (trends.collection_id â†’ data_collections.id)
- [ ] Check constraints work (try inserting invalid confidence_level, should fail)
- [ ] Unique constraints work (try creating duplicate username, should fail)
- [ ] Database health endpoint returns 200 OK with all tables listed
- [ ] Bootstrap user 'dave' created successfully
- [ ] Migration rollback works (`alembic downgrade base`, then `alembic upgrade head`)

### Automated Testing (Future)
```python
# tests/test_models.py
import pytest
from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker
from app.models import Base, Trend, DataCollection, User, ApiQuotaUsage


@pytest.fixture
async def test_engine():
    """Create test database engine."""
    engine = create_async_engine(
        "postgresql+asyncpg://localhost/trend_monitor_test",
        echo=True
    )
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    yield engine

    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.drop_all)
    await engine.dispose()


@pytest.mark.asyncio
async def test_create_trend(test_engine):
    """Test creating a trend record."""
    async_session = async_sessionmaker(test_engine, expire_on_commit=False)

    async with async_session() as session:
        # Create collection first
        collection = DataCollection(status="in_progress")
        session.add(collection)
        await session.flush()

        # Create trend
        trend = Trend(
            title="Test Trend",
            collection_id=collection.id,
            reddit_score=5000,
            momentum_score=85.5,
            confidence_level="high"
        )
        session.add(trend)
        await session.commit()

        assert trend.id is not None
        assert trend.title == "Test Trend"
        assert trend.momentum_score == 85.5
```

---

## Previous Story Intelligence

### Key Learnings from Story 1.1

**Successfully Implemented:**
1. âœ… Railway PostgreSQL provisioned and accessible
2. âœ… DATABASE_URL environment variable configured in Railway
3. âœ… SQLAlchemy 2.0.23 installed in requirements.txt
4. âœ… psycopg2-binary 2.9.9 installed
5. âœ… Configuration system using pydantic-settings functional
6. âœ… Backend directory structure: `backend/app/`
7. âœ… Dockerfile deployment working on Railway

**Deployment Pattern (from Story 1.1):**
- Dockerfile CMD: `alembic upgrade head && uvicorn app.main:app ...`
- This ensures migrations run automatically on each deployment

**Configuration Pattern (from Story 1.1):**
- Environment variables loaded via pydantic_settings
- Optional fields with defaults to prevent crashes
- Config accessed via `from app.config import settings`

**Files Created in Story 1.1:**
- backend/app/main.py - FastAPI app entry point
- backend/app/config.py - Configuration management
- backend/requirements.txt - Python dependencies
- Dockerfile - Container configuration

**Testing Pattern (from Story 1.1):**
- pytest and httpx installed for testing
- Test files in `backend/tests/`
- pytest.ini configuration exists

### How This Story Builds on Story 1.1

- **Database Connection**: Uses `settings.database_url` from config system created in 1.1
- **Directory Structure**: Follows `backend/app/` pattern from 1.1
- **Deployment**: Migrations run automatically via updated Dockerfile CMD from 1.1
- **Dependencies**: Extends requirements.txt from 1.1 with alembic and asyncpg
- **Testing**: Follows pytest pattern from 1.1

---

## Git Intelligence Summary

**Recent Commits Relevant to This Story:**
1. `fix: Code review fixes - config system, error handling, tests` (9843894)
   - Config system is functional and imported in main.py
   - Settings pattern established: `from app.config import settings`
   - All Settings fields are Optional with defaults

2. `fix: Remove HTTPSRedirectMiddleware` (0883106)
   - Railway handles HTTPS at edge level
   - Don't add HTTPS redirect in application

3. `fix: Use Railway PORT environment variable` (3e37366)
   - Dockerfile uses `${PORT:-8000}` pattern
   - Apply same pattern if adding any port-related config

**Code Patterns Established:**
- Import pattern: `from app.config import settings`
- Config access: `settings.database_url`, `settings.debug`
- Dockerfile CMD pattern: Run setup tasks then start app
- File structure: All app code in `backend/app/` directory
- Model imports: Use explicit imports in `__init__.py` for clean exports

---

## Latest Technical Information (Web Research)

### Alembic Best Practices (2026)

**Key Findings from Research:**

1. **Autogenerate with Manual Review** ([Alembic Tutorial](https://alembic.sqlalchemy.org/en/latest/tutorial.html))
   - Use `alembic revision --autogenerate` to generate migrations
   - ALWAYS manually review generated SQL before running
   - Autogenerate is not perfect - may miss some changes

2. **Naming Conventions** ([Best Practices Article](https://dev.to/welel/best-practices-for-alembic-and-sqlalchemy-3b34))
   - Define naming conventions in MetaData
   - Helps Alembic detect and manage constraints properly
   - Prevents "constraint not found" errors on downgrade

3. **Test Rollbacks** ([PingCAP Best Practices](https://www.pingcap.com/article/best-practices-alembic-schema-migration/))
   - Test `alembic downgrade` before production
   - Ensure rollbacks work without data loss
   - Critical for production safety

4. **Large Project Management** ([GitHub Discussion](https://github.com/sqlalchemy/alembic/discussions/1259))
   - For projects with 100+ migrations, periodically squash migrations
   - Create temporary alembic env, autogenerate against blank DB
   - Splice new files into main tree

### SQLAlchemy 2.0 Async Patterns (2026)

**Key Findings from Research:**

1. **AsyncAttrs Mixin** ([Litestar Docs](https://docs.litestar.dev/2/usage/databases/sqlalchemy/models_and_repository.html))
   - Inherit from `AsyncAttrs` for async attribute loading
   - Enables `await model.relationship` syntax
   - Required for relationships in async context

2. **UUID Primary Keys** ([SQLAlchemy Discussion](https://github.com/sqlalchemy/sqlalchemy/discussions/10698))
   - Use `default=uuid4` for application-side generation
   - Simpler than database-side generation
   - No need for PostgreSQL uuid-ossp extension

3. **Async Engine Configuration** ([GitHub Issue](https://github.com/sqlalchemy/sqlalchemy/issues/9808))
   - Use `create_async_engine` with asyncpg driver
   - Set `pool_pre_ping=True` for connection health checks
   - Use `NullPool` for serverless/short-lived connections

4. **mapped_column Syntax** ([Google Groups](https://groups.google.com/g/sqlalchemy/c/jsTxNpu7wPw))
   - Use `Mapped[Type]` type annotations with `mapped_column`
   - Replaces old `Column` syntax
   - Better type safety and IDE support

---

## Project Context Reference

**Project:** trend-monitor
**Project Type:** Quantified trend monitoring system with multi-API data collection
**User:** dave (content planning lead, non-technical)
**Goal:** Enable data-driven content planning decisions by detecting cross-platform trend momentum

**Database Purpose:**
- Store trend data from 4 APIs (Reddit, YouTube, Google Trends, SimilarWeb)
- Track data collection runs and API usage for monitoring
- Store calculated momentum scores and confidence levels
- Support future AI brief generation and caching
- Enable Phase 2 multi-user expansion

**Success Criteria:**
- 70%+ hit rate on high-confidence trends
- Meeting prep time: 2 hours â†’ 15 minutes
- Data collection: <30 minutes
- Dashboard load: <2 seconds
- Database queries: <500ms for dashboard views

---

## Definition of Done

This story is **DONE** when:

1. âœ… Alembic installed and initialized in backend directory
2. âœ… All 4 SQLAlchemy models created (Trend, DataCollection, User, ApiQuotaUsage)
3. âœ… Base model created with async support (AsyncAttrs, DeclarativeBase)
4. âœ… Initial migration generated with autogenerate
5. âœ… Migration reviewed and verified manually
6. âœ… Migration executed on Railway PostgreSQL database
7. âœ… All 4 tables created in Railway database
8. âœ… All indexes created (trends.momentum_score DESC, created_at DESC, confidence_level, collection_id, data_collections.started_at DESC, users.username)
9. âœ… Foreign key constraint created (trends.collection_id â†’ data_collections.id with CASCADE delete)
10. âœ… Check constraints created (confidence_level IN ('high', 'medium', 'low'), status IN ('in_progress', 'completed', 'failed'))
11. âœ… Unique constraints created (users.username, api_quota_usage(api_name, date))
12. âœ… Database session helper created (database.py with get_db dependency)
13. âœ… Database health endpoint created and returns all tables
14. âœ… Bootstrap user 'dave' created in users table
15. âœ… Dockerfile updated to run migrations automatically on deployment
16. âœ… Schema verified in Railway PostgreSQL via `railway run psql`
17. âœ… No errors in Railway logs after deployment with migrations
18. âœ… Story documentation updated with actual implementation

---

## Dev Agent Record

### Agent Model Used

**Claude Sonnet 4.5** (claude-sonnet-4-5-20250929)

### Completion Notes

**Implementation Summary:**
All database schema code successfully implemented and committed. Initial implementation completed in 3 commits:
1. `b760c28` - Initial implementation with all models, migrations, and database helpers
2. `a6b02c9` - Fixed database module to handle missing DATABASE_URL gracefully
3. `f39d30a` - Added URL conversion for asyncpg format (postgresql+asyncpg://)

Additional commits for verification:
4. `9c261d4` - Added schema verification script
5. `6af5545` - Added debug endpoint for schema verification
6. `e0bd740` - Fixed debug module import

**Final Status:** âœ… COMPLETE - All acceptance criteria met
**Deployment Status:** âœ… DEPLOYED - Railway deployment successful
**Schema Status:** âœ… VERIFIED - All tables, indexes, constraints, and foreign keys confirmed

**Verification Results (via /debug/schema endpoint):**
- âœ… All 4 tables created: trends, data_collections, users, api_quota_usage
- âœ… All required indexes created: idx_momentum_score_desc, idx_created_at_desc, idx_confidence_level, idx_started_at_desc
- âœ… Foreign key constraint: trends.collection_id â†’ data_collections.id (with CASCADE delete)
- âœ… Check constraints: confidence_level IN ('high', 'medium', 'low'), status IN ('in_progress', 'completed', 'failed')
- âœ… Unique constraints: users.username, api_quota_usage(api_name, date)
- âœ… Bootstrap user 'dave' exists (created: 2026-01-09 10:43:36)

**Resolved Issues:**
- Initial deployment 502 errors were resolved by bcrypt dependency fix in Story 1.3
- Schema verification completed successfully via temporary debug endpoint
- All database schema requirements from acceptance criteria fully satisfied

### Files Created/Modified

**Files Created:**
- backend/alembic.ini - Alembic configuration
- backend/alembic/env.py - Alembic environment with async support
- backend/alembic/versions/001_initial_schema.py - Initial migration
- backend/app/models/__init__.py - Models module exports
- backend/app/models/base.py - Base model with AsyncAttrs
- backend/app/models/trend.py - Trend model
- backend/app/models/data_collection.py - DataCollection model
- backend/app/models/user.py - User model
- backend/app/models/api_quota_usage.py - ApiQuotaUsage model
- backend/app/database.py - Database session management
- backend/scripts/verify_schema.py - Schema verification script (for future reference)
- backend/app/api/debug.py - Debug endpoint for schema verification (temporary)

**Files Modified:**
- backend/requirements.txt - Added alembic==1.13.1, asyncpg==0.29.0
- backend/app/config.py - Added database_url_async property for asyncpg URL format conversion
- backend/app/main.py - Added /health/db endpoint, lifespan events, database imports, debug router
- Dockerfile - Updated CMD to run migrations before starting app

**Implementation Details:**
- All 4 models created with proper async support (AsyncAttrs, TYPE_CHECKING for circular imports)
- Migration file manually created after autogenerate failed due to local db unavailability
- Database module includes None-checking for graceful degradation if DATABASE_URL not set
- Config property converts Railway's postgresql:// to postgresql+asyncpg:// format
- All indexes, check constraints, foreign keys, and unique constraints included in migration
- Schema verification endpoint created to validate all database elements

**Git Commits:**
- b760c28: feat: Add database schema with Alembic migrations
- a6b02c9: fix: Handle missing DATABASE_URL gracefully in database module
- f39d30a: fix: Convert Railway PostgreSQL URL to asyncpg format
- 9c261d4: Add database schema verification script
- 6af5545: Add debug endpoint for schema verification
- e0bd740: Fix debug module import

---

## Notes for Next Stories

**Story 1.3 (Backend Authentication)** will use:
- `User` model created in this story
- `get_db()` dependency created in this story
- Database session pattern established in this story

**Story 2.1 (API Collector Infrastructure)** will use:
- `DataCollection` model for tracking collection runs
- `ApiQuotaUsage` model for tracking API usage
- `Trend` model for storing collected data

**Story 2.2-2.5 (Data Collection)** will:
- Insert records into `trends` table
- Create `DataCollection` records to track runs
- Update `ApiQuotaUsage` for quota tracking

**Story 3.1 (Scoring Algorithm)** will:
- Read from `trends` table
- Update normalized score columns
- Update `momentum_score` and `confidence_level` columns

---

**Story Status:** âœ… Ready for Development
**Last Updated:** 2026-01-08

**Web Research Sources:**
- [Alembic Tutorial](https://alembic.sqlalchemy.org/en/latest/tutorial.html)
- [Best Practices for Alembic and SQLAlchemy](https://dev.to/welel/best-practices-for-alembic-and-sqlalchemy-3b34)
- [SQLAlchemy Models & Repository - Litestar Framework](https://docs.litestar.dev/2/usage/databases/sqlalchemy/models_and_repository.html)
- [SQLAlchemy UUID Primary Key Discussion](https://github.com/sqlalchemy/sqlalchemy/discussions/10698)
- [Best Practices for Alembic Schema Migration - PingCAP](https://www.pingcap.com/article/best-practices-alembic-schema-migration/)
